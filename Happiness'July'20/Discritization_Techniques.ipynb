{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct Bayesian Networks on our world happiness dataset, we first had to discritize all the continous features. In the past different types of discritization techniques viz. Supervised, Manual and Unsupervised have been explored and conclusions suggest that supervised algorithms such as the Fayyad and Irani (1993)http://refhub.elsevier.com/S1364-8152(17)31326-9/sref9 and Kononenko (1995)http://refhub.elsevier.com/S1364-8152(17)31326-9/sref14 which are based on maximizing predictability of the output variable generally outperform Unsupervised methods (which perform discritization on the basis of distributions). However, a downside of using supervised techniques are the resultant large CPTs(Conditional Probability Tables). On the other hand manual discritization offers greater flexibility in selecting appropriate intervals which are also physically interpretable but might not offer the best predictive power. To address this tradeoff we used a supervised learning alogrithm and then manually removed extra bins which were found to be unessential as per the literature thus preventing a large CPT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Fayyad and Irani's MDLP criterion discretiation algorithm\n",
    "https://github.com/navicto/Discretization-MDLPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "__author__ = 'Victor Ruiz, vmr11@pitt.edu'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "def entropy_numpy(data_classes, base=2):\n",
    "    '''\n",
    "    Computes the entropy of a set of labels (class instantiations)\n",
    "    :param base: logarithm base for computation\n",
    "    :param data_classes: Series with labels of examples in a dataset\n",
    "    :return: value of entropy\n",
    "    '''\n",
    "    classes = np.unique(data_classes)\n",
    "    N = len(data_classes)\n",
    "    ent = 0  # initialize entropy\n",
    "\n",
    "    # iterate over classes\n",
    "    for c in classes:\n",
    "        partition = data_classes[data_classes == c]  # data with class = c\n",
    "        proportion = len(partition) / N\n",
    "        #update entropy\n",
    "        ent -= proportion * log(proportion, base)\n",
    "\n",
    "    return ent\n",
    "\n",
    "def cut_point_information_gain_numpy(X, y, cut_point):\n",
    "    '''\n",
    "    Return de information gain obtained by splitting a numeric attribute in two according to cut_point\n",
    "    :param dataset: pandas dataframe with a column for attribute values and a column for class\n",
    "    :param cut_point: threshold at which to partition the numeric attribute\n",
    "    :param feature_label: column label of the numeric attribute values in data\n",
    "    :param class_label: column label of the array of instance classes\n",
    "    :return: information gain of partition obtained by threshold cut_point\n",
    "    '''\n",
    "    entropy_full = entropy_numpy(y)  # compute entropy of full dataset (w/o split)\n",
    "\n",
    "    #split data at cut_point\n",
    "    data_left_mask = X <= cut_point #dataset[dataset[feature_label] <= cut_point]\n",
    "    data_right_mask = X > cut_point #dataset[dataset[feature_label] > cut_point]\n",
    "    (N, N_left, N_right) = (len(X), data_left_mask.sum(), data_right_mask.sum())\n",
    "\n",
    "    gain = entropy_full - (N_left / N) * entropy_numpy(y[data_left_mask]) - \\\n",
    "        (N_right / N) * entropy_numpy(y[data_right_mask])\n",
    "\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "__author__ = 'Victor Ruiz, vmr11@pitt.edu'\n",
    "import numpy as np\n",
    "#from Entropy import entropy_numpy, cut_point_information_gain_numpy\n",
    "from math import log\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def previous_item(a, val):\n",
    "    idx = np.where(a == val)[0][0] - 1\n",
    "    return a[idx]\n",
    "\n",
    "class MDLP_Discretizer(TransformerMixin):\n",
    "    def __init__(self, features=None, raw_data_shape=None):\n",
    "        '''\n",
    "        initializes discretizer object:\n",
    "            saves raw copy of data and creates self._data with only features to discretize and class\n",
    "            computes initial entropy (before any splitting)\n",
    "            self._features = features to be discretized\n",
    "            self._classes = unique classes in raw_data\n",
    "            self._class_name = label of class in pandas dataframe\n",
    "            self._data = partition of data with only features of interest and class\n",
    "            self._cuts = dictionary with cut points for each feature\n",
    "        :param X: pandas dataframe with data to discretize\n",
    "        :param class_label: name of the column containing class in input dataframe\n",
    "        :param features: if !None, features that the user wants to discretize specifically\n",
    "        :return:\n",
    "        '''\n",
    "        #Initialize descriptions of discretizatino bins\n",
    "        self._bin_descriptions = {}\n",
    "\n",
    "        #Create array with attr indices to discretize\n",
    "        if features is None:  # Assume all columns are numeric and need to be discretized\n",
    "            if raw_data_shape is None:\n",
    "                raise Exception(\"If feautes=None, raw_data_shape must be a non-empty tuple\")\n",
    "            self._col_idx = range(raw_data_shape[1])\n",
    "        else:\n",
    "            if not isinstance(features, np.ndarray):\n",
    "                features = np.array(features)\n",
    "            if np.issubdtype(features.dtype, np.integer):\n",
    "                self._col_idx = features\n",
    "            elif np.issubdtype(features.dtype, np.bool):  # features passed as mask\n",
    "                if raw_data_shape is None:\n",
    "                    raise Exception('If features is a boolean array, raw_data_shape must be != None')\n",
    "                if len(features) != self._data_raw.shape[1]:\n",
    "                    raise Exception('Column boolean mask must be of dimensions (NColumns,)')\n",
    "                self._col_idx = np.where(features)\n",
    "            else:\n",
    "                raise Exception('features argument must a np.array of column indices or a boolean mask')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._data_raw = X  # copy of original input data\n",
    "        self._class_labels = y.reshape(-1, 1)  # make sure class labels is a column vector\n",
    "        self._classes = np.unique(self._class_labels)\n",
    "\n",
    "\n",
    "        if len(self._col_idx) != self._data_raw.shape[1]:  # some columns will not be discretized\n",
    "            self._ignore_col_idx = np.array([var for var in range(self._data_raw.shape[1]) if var not in self._col_idx])\n",
    "\n",
    "        # initialize feature bins cut points\n",
    "        self._cuts = {f: [] for f in self._col_idx}\n",
    "\n",
    "        # pre-compute all boundary points in dataset\n",
    "        self._boundaries = self.compute_boundary_points_all_features()\n",
    "\n",
    "        # get cuts for all features\n",
    "        self.all_features_accepted_cutpoints()\n",
    "\n",
    "        #generate bin string descriptions\n",
    "        self.generate_bin_descriptions()\n",
    "\n",
    "        #Generate one-hot encoding schema\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, inplace=False):\n",
    "        if inplace:\n",
    "            discretized = X\n",
    "        else:\n",
    "            discretized = X.copy()\n",
    "        discretized = self.apply_cutpoints(discretized)\n",
    "        return discretized\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, inplace=True)\n",
    "\n",
    "    def MDLPC_criterion(self, X, y, feature_idx, cut_point):\n",
    "        '''\n",
    "        Determines whether a partition is accepted according to the MDLPC criterion\n",
    "        :param feature: feature of interest\n",
    "        :param cut_point: proposed cut_point\n",
    "        :param partition_index: index of the sample (dataframe partition) in the interval of interest\n",
    "        :return: True/False, whether to accept the partition\n",
    "        '''\n",
    "        #get dataframe only with desired attribute and class columns, and split by cut_point\n",
    "        left_mask = X <= cut_point\n",
    "        right_mask = X > cut_point\n",
    "\n",
    "        #compute information gain obtained when splitting data at cut_point\n",
    "        cut_point_gain = cut_point_information_gain_numpy(X, y, cut_point)\n",
    "        #compute delta term in MDLPC criterion\n",
    "        N = len(X) # number of examples in current partition\n",
    "        partition_entropy = entropy_numpy(y)\n",
    "        k = len(np.unique(y))\n",
    "        k_left = len(np.unique(y[left_mask]))\n",
    "        k_right = len(np.unique(y[right_mask]))\n",
    "        entropy_left = entropy_numpy(y[left_mask])  # entropy of partition\n",
    "        entropy_right = entropy_numpy(y[right_mask])\n",
    "        delta = log(3 ** k, 2) - (k * partition_entropy) + (k_left * entropy_left) + (k_right * entropy_right)\n",
    "\n",
    "        #to split or not to split\n",
    "        gain_threshold = (log(N - 1, 2) + delta) / N\n",
    "\n",
    "        if cut_point_gain > gain_threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def feature_boundary_points(self, values):\n",
    "        '''\n",
    "        Given an attribute, find all potential cut_points (boundary points)\n",
    "        :param feature: feature of interest\n",
    "        :param partition_index: indices of rows for which feature value falls whithin interval of interest\n",
    "        :return: array with potential cut_points\n",
    "        '''\n",
    "\n",
    "        missing_mask = np.isnan(values)\n",
    "        data_partition = np.concatenate([values[:, np.newaxis], self._class_labels], axis=1)\n",
    "        data_partition = data_partition[~missing_mask]\n",
    "        #sort data by values\n",
    "        data_partition = data_partition[data_partition[:, 0].argsort()]\n",
    "\n",
    "        #Get unique values in column\n",
    "        unique_vals = np.unique(data_partition[:, 0])  # each of this could be a bin boundary\n",
    "        #Find if when feature changes there are different class values\n",
    "        boundaries = []\n",
    "        for i in range(1, unique_vals.size):  # By definition first unique value cannot be a boundary\n",
    "            previous_val_idx = np.where(data_partition[:, 0] == unique_vals[i-1])[0]\n",
    "            current_val_idx = np.where(data_partition[:, 0] == unique_vals[i])[0]\n",
    "            merged_classes = np.union1d(data_partition[previous_val_idx, 1], data_partition[current_val_idx, 1])\n",
    "            if merged_classes.size > 1:\n",
    "                boundaries += [unique_vals[i]]\n",
    "        boundaries_offset = np.array([previous_item(unique_vals, var) for var in boundaries])\n",
    "        return (np.array(boundaries) + boundaries_offset) / 2\n",
    "\n",
    "    def compute_boundary_points_all_features(self):\n",
    "        '''\n",
    "        Computes all possible boundary points for each attribute in self._features (features to discretize)\n",
    "        :return:\n",
    "        '''\n",
    "        def padded_cutpoints_array(arr, N):\n",
    "            cutpoints = self.feature_boundary_points(arr)\n",
    "            padding = np.array([np.nan] * (N - len(cutpoints)))\n",
    "            return np.concatenate([cutpoints, padding])\n",
    "\n",
    "        boundaries = np.empty(self._data_raw.shape)\n",
    "        boundaries[:, self._col_idx] = np.apply_along_axis(padded_cutpoints_array, 0, self._data_raw[:, self._col_idx], self._data_raw.shape[0])\n",
    "        mask = np.all(np.isnan(boundaries), axis=1)\n",
    "        return boundaries[~mask]\n",
    "\n",
    "    def boundaries_in_partition(self, X, feature_idx):\n",
    "        '''\n",
    "        From the collection of all cut points for all features, find cut points that fall within a feature-partition's\n",
    "        attribute-values' range\n",
    "        :param data: data partition (pandas dataframe)\n",
    "        :param feature: attribute of interest\n",
    "        :return: points within feature's range\n",
    "        '''\n",
    "        range_min, range_max = (X.min(), X.max())\n",
    "        mask = np.logical_and((self._boundaries[:, feature_idx] > range_min), (self._boundaries[:, feature_idx] < range_max))\n",
    "        return np.unique(self._boundaries[:, feature_idx][mask])\n",
    "\n",
    "    def best_cut_point(self, X, y, feature_idx):\n",
    "        '''\n",
    "        Selects the best cut point for a feature in a data partition based on information gain\n",
    "        :param data: data partition (pandas dataframe)\n",
    "        :param feature: target attribute\n",
    "        :return: value of cut point with highest information gain (if many, picks first). None if no candidates\n",
    "        '''\n",
    "        candidates = self.boundaries_in_partition(X, feature_idx=feature_idx)\n",
    "        if candidates.size == 0:\n",
    "            return None\n",
    "        gains = [(cut, cut_point_information_gain_numpy(X, y, cut_point=cut)) for cut in candidates]\n",
    "        gains = sorted(gains, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return gains[0][0] #return cut point\n",
    "\n",
    "    def single_feature_accepted_cutpoints(self, X, y, feature_idx):\n",
    "        '''\n",
    "        Computes the cuts for binning a feature according to the MDLP criterion\n",
    "        :param feature: attribute of interest\n",
    "        :param partition_index: index of examples in data partition for which cuts are required\n",
    "        :return: list of cuts for binning feature in partition covered by partition_index\n",
    "        '''\n",
    "\n",
    "        #Delte missing data\n",
    "        mask = np.isnan(X)\n",
    "        X = X[~mask]\n",
    "        y = y[~mask]\n",
    "\n",
    "        #stop if constant or null feature values\n",
    "        if len(np.unique(X)) < 2:\n",
    "            return\n",
    "        #determine whether to cut and where\n",
    "        cut_candidate = self.best_cut_point(X, y, feature_idx)\n",
    "        if cut_candidate == None:\n",
    "            return\n",
    "        decision = self.MDLPC_criterion(X, y, feature_idx, cut_candidate)\n",
    "\n",
    "        # partition masks\n",
    "        left_mask = X <= cut_candidate\n",
    "        right_mask = X > cut_candidate\n",
    "\n",
    "        #apply decision\n",
    "        if not decision:\n",
    "            return  # if partition wasn't accepted, there's nothing else to do\n",
    "        if decision:\n",
    "            #now we have two new partitions that need to be examined\n",
    "            left_partition = X[left_mask]\n",
    "            right_partition = X[right_mask]\n",
    "            if (left_partition.size == 0) or (right_partition.size == 0):\n",
    "                return #extreme point selected, don't partition\n",
    "            self._cuts[feature_idx] += [cut_candidate]  # accept partition\n",
    "            self.single_feature_accepted_cutpoints(left_partition, y[left_mask], feature_idx)\n",
    "            self.single_feature_accepted_cutpoints(right_partition, y[right_mask], feature_idx)\n",
    "            #order cutpoints in ascending order\n",
    "            self._cuts[feature_idx] = sorted(self._cuts[feature_idx])\n",
    "            return\n",
    "\n",
    "    def all_features_accepted_cutpoints(self):\n",
    "        '''\n",
    "        Computes cut points for all numeric features (the ones in self._features)\n",
    "        :return:\n",
    "        '''\n",
    "        for attr in self._col_idx:\n",
    "            self.single_feature_accepted_cutpoints(X=self._data_raw[:, attr], y=self._class_labels, feature_idx=attr)\n",
    "        return\n",
    "\n",
    "    def generate_bin_descriptions(self):\n",
    "        '''\n",
    "        Discretizes data by applying bins according to self._cuts. Saves a new, discretized file, and a description of\n",
    "        the bins\n",
    "        :param out_data_path: path to save discretized data\n",
    "        :param out_bins_path: path to save bins description\n",
    "        :return:\n",
    "        '''\n",
    "        bin_label_collection = {}\n",
    "        for attr in self._col_idx:\n",
    "            if len(self._cuts[attr]) == 0:\n",
    "                bin_label_collection[attr] = ['All']\n",
    "            else:\n",
    "                cuts = [-np.inf] + self._cuts[attr] + [np.inf]\n",
    "                start_bin_indices = range(0, len(cuts) - 1)\n",
    "                bin_labels = ['%s_to_%s' % (str(cuts[i]), str(cuts[i+1])) for i in start_bin_indices]\n",
    "                bin_label_collection[attr] = bin_labels\n",
    "                self._bin_descriptions[attr] = {i: bin_labels[i] for i in range(len(bin_labels))}\n",
    "\n",
    "\n",
    "    def apply_cutpoints(self, data):\n",
    "        '''\n",
    "        Discretizes data by applying bins according to self._cuts. Saves a new, discretized file, and a description of\n",
    "        the bins\n",
    "        :param out_data_path: path to save discretized data\n",
    "        :param out_bins_path: path to save bins description\n",
    "        :return:\n",
    "        '''\n",
    "        for attr in self._col_idx:\n",
    "            if len(self._cuts[attr]) == 0:\n",
    "                # data[:, attr] = 'All'\n",
    "                data[:, attr] = 0\n",
    "            else:\n",
    "                cuts = [-np.inf] + self._cuts[attr] + [np.inf]\n",
    "                discretized_col = np.digitize(x=data[:, attr], bins=cuts, right=False).astype('float') - 1\n",
    "                discretized_col[np.isnan(data[:, attr])] = np.nan\n",
    "                data[:, attr] = discretized_col\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log GDP per capita</th>\n",
       "      <th>Social support</th>\n",
       "      <th>Healthy life expectancy at birth</th>\n",
       "      <th>Freedom to make life choices</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Perceptions of corruption</th>\n",
       "      <th>Positive affect</th>\n",
       "      <th>Negative affect</th>\n",
       "      <th>Confidence in national government</th>\n",
       "      <th>Democratic Quality</th>\n",
       "      <th>Delivery Quality</th>\n",
       "      <th>gini of household income reported in Gallup, by wp5-year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.471376</td>\n",
       "      <td>0.559072</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.522566</td>\n",
       "      <td>0.053188</td>\n",
       "      <td>0.793246</td>\n",
       "      <td>0.564953</td>\n",
       "      <td>0.348332</td>\n",
       "      <td>0.324990</td>\n",
       "      <td>-1.855448</td>\n",
       "      <td>-1.394648</td>\n",
       "      <td>0.418629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.472197</td>\n",
       "      <td>0.490880</td>\n",
       "      <td>52.799999</td>\n",
       "      <td>0.427011</td>\n",
       "      <td>-0.110382</td>\n",
       "      <td>0.954393</td>\n",
       "      <td>0.496349</td>\n",
       "      <td>0.371326</td>\n",
       "      <td>0.261179</td>\n",
       "      <td>-1.896539</td>\n",
       "      <td>-1.440218</td>\n",
       "      <td>0.286599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.458603</td>\n",
       "      <td>0.507516</td>\n",
       "      <td>52.599998</td>\n",
       "      <td>0.373536</td>\n",
       "      <td>-0.082319</td>\n",
       "      <td>0.927606</td>\n",
       "      <td>0.424125</td>\n",
       "      <td>0.404904</td>\n",
       "      <td>0.364666</td>\n",
       "      <td>-1.870725</td>\n",
       "      <td>-1.438761</td>\n",
       "      <td>0.290681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.458469</td>\n",
       "      <td>0.419973</td>\n",
       "      <td>52.400002</td>\n",
       "      <td>0.393656</td>\n",
       "      <td>-0.096549</td>\n",
       "      <td>0.923849</td>\n",
       "      <td>0.351387</td>\n",
       "      <td>0.502474</td>\n",
       "      <td>0.341482</td>\n",
       "      <td>-1.874237</td>\n",
       "      <td>-1.424542</td>\n",
       "      <td>0.374930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.337564</td>\n",
       "      <td>0.638411</td>\n",
       "      <td>68.099998</td>\n",
       "      <td>0.729819</td>\n",
       "      <td>-0.017473</td>\n",
       "      <td>0.901071</td>\n",
       "      <td>0.675244</td>\n",
       "      <td>0.321706</td>\n",
       "      <td>0.400910</td>\n",
       "      <td>0.257297</td>\n",
       "      <td>-0.132960</td>\n",
       "      <td>0.416540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Log GDP per capita  Social support  Healthy life expectancy at birth  \\\n",
       "0            7.471376        0.559072                         53.000000   \n",
       "1            7.472197        0.490880                         52.799999   \n",
       "2            7.458603        0.507516                         52.599998   \n",
       "3            7.458469        0.419973                         52.400002   \n",
       "4            9.337564        0.638411                         68.099998   \n",
       "\n",
       "   Freedom to make life choices  Generosity  Perceptions of corruption  \\\n",
       "0                      0.522566    0.053188                   0.793246   \n",
       "1                      0.427011   -0.110382                   0.954393   \n",
       "2                      0.373536   -0.082319                   0.927606   \n",
       "3                      0.393656   -0.096549                   0.923849   \n",
       "4                      0.729819   -0.017473                   0.901071   \n",
       "\n",
       "   Positive affect  Negative affect  Confidence in national government  \\\n",
       "0         0.564953         0.348332                           0.324990   \n",
       "1         0.496349         0.371326                           0.261179   \n",
       "2         0.424125         0.404904                           0.364666   \n",
       "3         0.351387         0.502474                           0.341482   \n",
       "4         0.675244         0.321706                           0.400910   \n",
       "\n",
       "   Democratic Quality  Delivery Quality  \\\n",
       "0           -1.855448         -1.394648   \n",
       "1           -1.896539         -1.440218   \n",
       "2           -1.870725         -1.438761   \n",
       "3           -1.874237         -1.424542   \n",
       "4            0.257297         -0.132960   \n",
       "\n",
       "   gini of household income reported in Gallup, by wp5-year  \n",
       "0                                           0.418629         \n",
       "1                                           0.286599         \n",
       "2                                           0.290681         \n",
       "3                                           0.374930         \n",
       "4                                           0.416540         "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('BN_continous_data.csv')\n",
    "dataset_y = dataset['Life Ladder']\n",
    "dataset_x = dataset.drop(['Life Ladder'],axis=1)\n",
    "dataset_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1     2     3     4      5     6     7     8     9    10    11  \\\n",
      "0  41.0  56.0  42.0  71.0  99.0   97.0  68.0   1.0  23.0  48.0  43.0  50.0   \n",
      "1  84.0  97.0  74.0  47.0  29.0  101.0  69.0  45.0   8.0  89.0  81.0   2.0   \n",
      "2  18.0   8.0   1.0  46.0  62.0   43.0  51.0  93.0  90.0  28.0  33.0  88.0   \n",
      "3  83.0  94.0  78.0  99.0  63.0   68.0  23.0  60.0   9.0  93.0  87.0  22.0   \n",
      "4  21.0   8.0   3.0  26.0  58.0   52.0  44.0  96.0  64.0  31.0  38.0  81.0   \n",
      "\n",
      "   Life Ladder  \n",
      "0     6.425144  \n",
      "1     6.243429  \n",
      "2     4.542546  \n",
      "3     6.166838  \n",
      "4     5.268375  \n",
      "      0     1     2     3     4     5     6     7      8     9    10    11  \\\n",
      "0  22.0  38.0  20.0  12.0  40.0  40.0  47.0  49.0   23.0  23.0  22.0  64.0   \n",
      "1   9.0  14.0  29.0  96.0  72.0   0.0  70.0  60.0  106.0  28.0  75.0  99.0   \n",
      "2  48.0  15.0  71.0  64.0  69.0  92.0  55.0  76.0   45.0  69.0  57.0  63.0   \n",
      "3  39.0  35.0  58.0  31.0  21.0  37.0  28.0  86.0   56.0  30.0  72.0  27.0   \n",
      "4  24.0  45.0  42.0   9.0  29.0  62.0  16.0  93.0   50.0   3.0  48.0  49.0   \n",
      "\n",
      "   Life Ladder  \n",
      "0     4.152619  \n",
      "1     3.332990  \n",
      "2     5.004403  \n",
      "3     4.452548  \n",
      "4     4.906618  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from MDLP import MDLP_Discretizer\n",
    "\n",
    "def main():\n",
    "\n",
    "    ######### USE-CASE EXAMPLE #############\n",
    "\n",
    "    #read dataset\n",
    "    dataset = pd.read_csv('BN_continous_data.csv')\n",
    "    dataset_y = dataset['Life Ladder']\n",
    "    dataset_x = dataset.drop(['Life Ladder'],axis=1)\n",
    "    #dataset_x.head()\n",
    "    dataset_x= dataset_x.to_numpy()\n",
    "    dataset_y= dataset_y.to_numpy()\n",
    "    #feature_names, class_names = dataset['feature_names'], dataset['target_names']\n",
    "    numeric_features = np.arange(dataset_x.shape[1])  # all features in this dataset are numeric. These will be discretized\n",
    "\n",
    "    #Split between training and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset_x,dataset_y, test_size=0.33)\n",
    "\n",
    "    #Initialize discretizer object and fit to training data\n",
    "    discretizer = MDLP_Discretizer(features=numeric_features)\n",
    "    discretizer.fit(X_train, y_train)\n",
    "    X_train_discretized = discretizer.transform(X_train)\n",
    "    #apply same discretization to test set\n",
    "    X_test_discretized = discretizer.transform(X_test)\n",
    "    pd_x_train_dis= pd.DataFrame(X_train_discretized)\n",
    "    pd_x_train_dis['Life Ladder'] = y_train \n",
    "    print(pd_x_train_dis.head())\n",
    "    pd_x_test_dis= pd.DataFrame(X_test_discretized)\n",
    "    pd_x_test_dis['Life Ladder'] = y_test\n",
    "    print(pd_x_test_dis.head())\n",
    "    df_row_reindex = pd.concat([pd_x_train_dis,pd_x_test_dis], ignore_index=True)\n",
    "    df_row_reindex.to_csv('BN_data_dis.csv',index=False)\n",
    "    #Print a slice of original and discretized data\n",
    "    #print('Original dataset:\\n%s' % str(X_train[0:5]))\n",
    "    #print('Discretized dataset:\\n%s' % str(X_train_discretized[0:5]))\n",
    "\n",
    "    #see how feature 0 was discretized\n",
    "    #print('Feature: %s' % feature_names[0])\n",
    "    #print('Interval cut-points: %s' % str(discretizer._cuts[0]))\n",
    "    #print('Bin descriptions: %s' % str(discretizer._bin_descriptions[0]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_discretized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9f49c2fdfec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd_x_train_dis\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_discretized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd_x_train_dis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Life Ladder'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpd_x_train_dis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_discretized' is not defined"
     ]
    }
   ],
   "source": [
    "pd_x_train_dis= pd.DataFrame(X_train_discretized)\n",
    "pd_x_train_dis['Life Ladder'] = y_train \n",
    "print(pd_x_train_dis.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_x_test_dis= pd.DataFrame(X_test_discretized)\n",
    "pd_x_test_dis['Life Ladder'] = y_test\n",
    "print(pd_x_test_dis.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
